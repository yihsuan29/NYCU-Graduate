# Project Overview

## ML Homework 1: Regularized Polynomial Regression
Implemented regularized polynomial linear regression using closed-form LSE, steepest descent, and Newton’s method to fit and visualize curves on 1D regression datasets.

## ML Homework 2: Naive Bayes and Online Learning
Implemented discrete and continuous Naive Bayes classifiers for handwritten digit classification on MNIST, and online Bayesian learning to estimate coin-toss probabilities using Beta distributions.

## ML Homework 3: Random Data Generation and Sequential Estimation
Implemented Gaussian and polynomial data generators and applied sequential estimation to iteratively estimate the mean and variance of univariate Gaussian data.

## ML Homework 4: Logistic Regression and EM Algorithm
Implemented logistic regression with gradient descent and Newton’s method for synthetic binary classification, and applied the EM algorithm for Bernoulli mixture clustering on MNIST.

## ML Homework 5: Gaussian Process and Support Vector Machines
Applied Gaussian Process Regression with rational quadratic kernels for function prediction and uncertainty estimation, and implemented SVM classifiers for digit classification on MNIST (digits 0–4).

## ML Homework 6: Kernel K-means and Spectral Clustering
Implemented kernel k-means and spectral clustering (ratio cut and normalized cut) using a spatial-color RBF kernel to segment image pixels into clusters.

## ML Homework 7: Kernel Eigenfaces and t-SNE
Implemented PCA, LDA, kernel PCA, and kernel LDA for face recognition on the Yale Face Dataset, and analyzed t-SNE versus symmetric SNE for MNIST visualization.